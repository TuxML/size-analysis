{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_selection import FeaturesLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test FeatureLoader with the list of features importances in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_importance.csv',\n",
       " 'feature_importanceDT.csv',\n",
       " 'feature_importanceEN.csv',\n",
       " 'feature_importanceGB.csv',\n",
       " 'feature_importanceLasso.csv',\n",
       " 'feature_importanceLR.csv',\n",
       " 'feature_importanceRF-415.csv',\n",
       " 'feature_importanceRF-ccoptimize.csv',\n",
       " 'feature_importanceRF-GZIP.csv',\n",
       " 'feature_importanceRF.csv',\n",
       " 'feature_importanceRidge.csv',\n",
       " 'correlations_vmlinux.csv',\n",
       " 'feature_net.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "file_list = os.listdir(\".\")\n",
    "res_list = [*map(lambda x : re.search(\"feature_imp.*csv\",x), file_list)]\n",
    "csv_list = [file_list[k] for k in np.where([res !=  None for res in res_list])[0]]\n",
    "csv_list.append('correlations_vmlinux.csv')\n",
    "csv_list.append('feature_net.csv')\n",
    "csv_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_importance.csv  => ok!\n",
      "feature_importanceDT.csv  => ok!\n",
      "feature_importanceEN.csv  => ok!\n",
      "feature_importanceGB.csv  => ok!\n",
      "feature_importanceLasso.csv  => ok!\n",
      "feature_importanceLR.csv  => ok!\n",
      "feature_importanceRF-415.csv is not loaded correctly\n",
      "feature_importanceRF-ccoptimize.csv  => ok!\n",
      "feature_importanceRF-GZIP.csv  => ok!\n",
      "feature_importanceRF.csv  => ok!\n",
      "feature_importanceRidge.csv  => ok!\n",
      "correlations_vmlinux.csv  => ok!\n",
      "feature_net.csv  => ok!\n"
     ]
    }
   ],
   "source": [
    "for c in csv_list:\n",
    "    try:\n",
    "        f= FeaturesLoader(c)\n",
    "        a = f.get_selected_features()\n",
    "        print(c, \" => ok!\")\n",
    "    except:\n",
    "        print(c, \"is not loaded correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still working on the new features_importanceRF[415/cc/GZIP].csv, the tests are false for those three (default != RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAUNCHER \n",
    "\n",
    "=> need tensorflow with a GPU & Cuda+Cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from net_launcher import NetLauncher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = NetLauncher(name_csv = 'feature_net.csv', predict_var ='vmlinux', drop_feature = False,\n",
    "                 nb_features = 1000, learning_rate1 = 0.5, learning_rate2 = 0.025, nb_node_layer1 = 200,\n",
    "                 nb_node_layer2 = 300, batch_size = 50, nb_epochs = 30, training_size = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the parameters:\n",
    "- name_csv is the name of the csv used to select one sample of features\n",
    "- predict_var is the variable we try to predict, here vmlinux\n",
    "- drop_feature is a boolean: True if we use the full list of features present in the csv, false otherwise\n",
    "- nb_features is the number of features we want to keep. It should select the nb_features best features automatically\n",
    "- learning_rate1, learning_rate2, nb_node_layer1, nb_node_layer2, batch_size, nb_epochs are the parameters of the neural network\n",
    "- training_size is the proportion of configurations kept for the training\n",
    "\n",
    "About the output:\n",
    "- the first number is the final training MAPE\n",
    "- the second number is the final testing MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final cost = 3.5387111072866326\n",
      "Test final cost = 5.706194976659922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.5387111072866326, 5.706194976659922)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
