{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_selection import FeaturesLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test FeatureLoader with the list of features importances in the folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['feature_importance.csv',\n",
       " 'feature_importanceDT.csv',\n",
       " 'feature_importanceEN.csv',\n",
       " 'feature_importanceGB.csv',\n",
       " 'feature_importanceLasso.csv',\n",
       " 'feature_importanceLR.csv',\n",
       " 'feature_importanceRF-415.csv',\n",
       " 'feature_importanceRF-ccoptimize.csv',\n",
       " 'feature_importanceRF-GZIP.csv',\n",
       " 'feature_importanceRF.csv',\n",
       " 'feature_importanceRidge.csv',\n",
       " 'correlations_vmlinux.csv',\n",
       " 'feature_net.csv']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "file_list = os.listdir(\".\")\n",
    "res_list = [*map(lambda x : re.search(\"feature_imp.*csv\",x), file_list)]\n",
    "csv_list = [file_list[k] for k in np.where([res !=  None for res in res_list])[0]]\n",
    "csv_list.append('correlations_vmlinux.csv')\n",
    "csv_list.append('feature_net.csv')\n",
    "csv_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_importance.csv  => ok!\n",
      "feature_importanceDT.csv  => ok!\n",
      "feature_importanceEN.csv  => ok!\n",
      "feature_importanceGB.csv  => ok!\n",
      "feature_importanceLasso.csv  => ok!\n",
      "feature_importanceLR.csv  => ok!\n",
      "feature_importanceRF-415.csv is not loaded correctly\n",
      "feature_importanceRF-ccoptimize.csv  => ok!\n",
      "feature_importanceRF-GZIP.csv  => ok!\n",
      "feature_importanceRF.csv  => ok!\n",
      "feature_importanceRidge.csv  => ok!\n",
      "correlations_vmlinux.csv  => ok!\n",
      "feature_net.csv  => ok!\n"
     ]
    }
   ],
   "source": [
    "for c in csv_list:\n",
    "    try:\n",
    "        f= FeaturesLoader(c)\n",
    "        a = f.get_selected_features()\n",
    "        print(c, \" => ok!\")\n",
    "    except:\n",
    "        print(c, \"is not loaded correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still working on the new features_importanceRF[415/cc/GZIP].csv, the tests are false for those three (default != RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAUNCHER \n",
    "\n",
    "=> need tensorflow with a GPU & Cuda+Cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from net_launcher import NetLauncher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = NetLauncher(name_csv = 'feature_net.csv', predict_var ='vmlinux', drop_feature = False,\n",
    "                 nb_features = 1000, learning_rate1 = 0.5, learning_rate2 = 0.025, nb_node_layer1 = 200,\n",
    "                 nb_node_layer2 = 300, batch_size = 50, nb_epochs = 30, training_size = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the parameters:\n",
    "- name_csv is the name of the csv used to select one sample of features\n",
    "- predict_var is the variable we try to predict, here vmlinux\n",
    "- drop_feature is a boolean: True if we use the full list of features present in the csv, false otherwise\n",
    "- nb_features is the number of features we want to keep. It should select the nb_features best features automatically\n",
    "- learning_rate1, learning_rate2, nb_node_layer1, nb_node_layer2, batch_size, nb_epochs are the parameters of the neural network\n",
    "- training_size is the proportion of configurations kept for the training\n",
    "\n",
    "About the output:\n",
    "- the first number is the final training MAPE\n",
    "- the second number is the final testing MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\llesoil\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\llesoil\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Training final cost = 3.5096331903899927\n",
      "Test final cost = 5.775507364584052\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.5096331903899927, 5.775507364584052)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\llesoil\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\llesoil\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Training final cost = 6.659049865128337\n",
      "Test final cost = 7.60940766075383\n",
      "Training final cost = 4.849796628606493\n",
      "Test final cost = 6.0387497803439265\n",
      "Training final cost = 4.310755191793764\n",
      "Test final cost = 5.709685911302981\n",
      "Training final cost = 3.9875450645379975\n",
      "Test final cost = 5.480758883382963\n",
      "Training final cost = 4.052626930285191\n",
      "Test final cost = 5.439690132503924\n",
      "Training final cost = 3.789690756423462\n",
      "Test final cost = 5.84771770498027\n",
      "Training final cost = 3.9891709793305052\n",
      "Test final cost = 6.8080461362133855\n",
      "Training final cost = 4.034539904283441\n",
      "Test final cost = 7.189294978328373\n",
      "Training final cost = 3.935554333643061\n",
      "Test final cost = 7.509617224983547\n",
      "Training final cost = 4.376758111271881\n",
      "Test final cost = 7.4890546798706055\n",
      "Training final cost = 4.5230775442388325\n",
      "Test final cost = 8.127344022626461\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot create a tensor proto whose content is larger than 2GB.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-78e3c2969fa2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                  \u001b[0mnb_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.025\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_node_layer1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                  nb_node_layer2 = 300, batch_size = 50, nb_epochs = 30, training_size = 0.9)\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\tuxml\\git\\size-analysis\\net_launcher.py\u001b[0m in \u001b[0;36mlaunch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_size\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m92\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_size\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m92\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_small\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\tuxml\\git\\size-analysis\\net_launcher.py\u001b[0m in \u001b[0;36mcompute_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;31m# slice the datasets => feed_dict was very slow, so I choose an iterator solution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mdataset_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[0miterator_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_initializable_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[0mxtr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterator_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m   1440\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1442\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mDatasetV1Adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    296\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \"\"\"\n\u001b[1;32m--> 298\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m   \u001b[1;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, tensors)\u001b[0m\n\u001b[0;32m   1875\u001b[0m           if sparse_tensor_lib.is_sparse(t) else ops.convert_to_tensor(\n\u001b[0;32m   1876\u001b[0m               t, name=\"component_%d\" % i)\n\u001b[1;32m-> 1877\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1878\u001b[0m       ])\n\u001b[0;32m   1879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1875\u001b[0m           if sparse_tensor_lib.is_sparse(t) else ops.convert_to_tensor(\n\u001b[0;32m   1876\u001b[0m               t, name=\"component_%d\" % i)\n\u001b[1;32m-> 1877\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1878\u001b[0m       ])\n\u001b[0;32m   1879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[0;32m   1037\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m   \"\"\"\n\u001b[1;32m-> 1039\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1095\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1097\u001b[1;33m       as_ref=False)\n\u001b[0m\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1175\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    302\u001b[0m                                          as_ref=False):\n\u001b[0;32m    303\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    243\u001b[0m   \"\"\"\n\u001b[0;32m    244\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[1;32m--> 245\u001b[1;33m                         allow_broadcast=True)\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    281\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m    282\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m           allow_broadcast=allow_broadcast))\n\u001b[0m\u001b[0;32m    284\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnparray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnparray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m<<\u001b[0m \u001b[1;36m31\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m       raise ValueError(\n\u001b[1;32m--> 537\u001b[1;33m           \"Cannot create a tensor proto whose content is larger than 2GB.\")\n\u001b[0m\u001b[0;32m    538\u001b[0m     \u001b[0mtensor_proto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnparray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot create a tensor proto whose content is larger than 2GB."
     ]
    }
   ],
   "source": [
    "nb_f = [100,200,300,400,500,1000,2000,3000,4000,5000,6000,7000,8000]\n",
    "for nb_features in nb_f:\n",
    "    n = NetLauncher(name_csv = 'feature_importanceRF.csv', predict_var ='vmlinux', drop_feature = True,\n",
    "                 nb_features = nb_features, learning_rate1 = 0.5, learning_rate2 = 0.025, nb_node_layer1 = 200,\n",
    "                 nb_node_layer2 = 300, batch_size = 50, nb_epochs = 30, training_size = 0.9)\n",
    "    n.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\llesoil\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\llesoil\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Training final cost = 13.69589406511058\n",
      "Test final cost = 14.959572960637527\n",
      "Training final cost = 13.58116999398107\n",
      "Test final cost = 14.09030630795852\n",
      "Training final cost = 5.351237390352332\n",
      "Test final cost = 9.288764971601383\n",
      "Training final cost = 4.799785811616027\n",
      "Test final cost = 8.60368048723625\n",
      "Training final cost = 4.427562525306922\n",
      "Test final cost = 7.639438644699428\n",
      "Training final cost = 4.40853732370812\n",
      "Test final cost = 6.973258264254833\n",
      "Training final cost = 4.26731964868048\n",
      "Test final cost = 6.712376810675082\n",
      "Training final cost = 4.103867670116217\n",
      "Test final cost = 6.265186765919561\n",
      "Training final cost = 4.1275388147818\n",
      "Test final cost = 6.025386233260666\n",
      "Training final cost = 4.06735273450613\n",
      "Test final cost = 5.652665975301162\n",
      "Training final cost = 3.979317093528987\n",
      "Test final cost = 5.6208383829697315\n",
      "Training final cost = 3.922811348505135\n",
      "Test final cost = 5.283837520557901\n"
     ]
    }
   ],
   "source": [
    "training_size = [0.05,0.1,0.15,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9, 0.95]\n",
    "for tr_size in training_size:\n",
    "    n = NetLauncher(name_csv = 'feature_importanceRF.csv', predict_var ='vmlinux', drop_feature = True,\n",
    "                 nb_features = 500, learning_rate1 = 0.5, learning_rate2 = 0.025, nb_node_layer1 = 200,\n",
    "                 nb_node_layer2 = 300, batch_size = 50, nb_epochs = 30, training_size = tr_size)\n",
    "    n.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final cost = 5.799691134028965\n",
      "Test final cost = 6.333040330720984\n",
      "Training final cost = 4.154822326368755\n",
      "Test final cost = 4.866754097783047\n",
      "Training final cost = 3.8376484984073085\n",
      "Test final cost = 4.715879854948624\n",
      "Training final cost = 3.320488642235309\n",
      "Test final cost = 4.139129569996959\n",
      "Training final cost = 3.078002595815106\n",
      "Test final cost = 3.9029799868231234\n",
      "Training final cost = 2.8879172374372897\n",
      "Test final cost = 3.82620145186134\n",
      "Training final cost = 2.550374199874735\n",
      "Test final cost = 3.599071497502534\n",
      "Training final cost = 1.9736762917848025\n",
      "Test final cost = 2.980320342856905\n",
      "Training final cost = 1.7838127887335375\n",
      "Test final cost = 2.8383495658636093\n",
      "Training final cost = 1.5636501600560935\n",
      "Test final cost = 2.7873511683681738\n",
      "Training final cost = 1.6732350484184597\n",
      "Test final cost = 2.9295172173043955\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot create a tensor proto whose content is larger than 2GB.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-1a5669db2348>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m                  \u001b[0mnb_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.025\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_node_layer1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                  nb_node_layer2 = 300, batch_size = 50, nb_epochs = 30, training_size = 0.9)\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\tuxml\\git\\size-analysis\\net_launcher.py\u001b[0m in \u001b[0;36mlaunch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_size\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m92\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_size\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m92\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_small\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\tuxml\\git\\size-analysis\\net_launcher.py\u001b[0m in \u001b[0;36mcompute_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;31m# slice the datasets => feed_dict was very slow, so I choose an iterator solution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mdataset_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[0miterator_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_initializable_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[0mxtr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterator_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m   1440\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1442\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mDatasetV1Adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    296\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \"\"\"\n\u001b[1;32m--> 298\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m   \u001b[1;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, tensors)\u001b[0m\n\u001b[0;32m   1875\u001b[0m           if sparse_tensor_lib.is_sparse(t) else ops.convert_to_tensor(\n\u001b[0;32m   1876\u001b[0m               t, name=\"component_%d\" % i)\n\u001b[1;32m-> 1877\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1878\u001b[0m       ])\n\u001b[0;32m   1879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1875\u001b[0m           if sparse_tensor_lib.is_sparse(t) else ops.convert_to_tensor(\n\u001b[0;32m   1876\u001b[0m               t, name=\"component_%d\" % i)\n\u001b[1;32m-> 1877\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1878\u001b[0m       ])\n\u001b[0;32m   1879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[0;32m   1037\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m   \"\"\"\n\u001b[1;32m-> 1039\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1095\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1097\u001b[1;33m       as_ref=False)\n\u001b[0m\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx, accept_symbolic_tensors)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1175\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    302\u001b[0m                                          as_ref=False):\n\u001b[0;32m    303\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    243\u001b[0m   \"\"\"\n\u001b[0;32m    244\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[1;32m--> 245\u001b[1;33m                         allow_broadcast=True)\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    281\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m    282\u001b[0m           \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m           allow_broadcast=allow_broadcast))\n\u001b[0m\u001b[0;32m    284\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[1;32m~\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    535\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnparray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnparray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m<<\u001b[0m \u001b[1;36m31\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m       raise ValueError(\n\u001b[1;32m--> 537\u001b[1;33m           \"Cannot create a tensor proto whose content is larger than 2GB.\")\n\u001b[0m\u001b[0;32m    538\u001b[0m     \u001b[0mtensor_proto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnparray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot create a tensor proto whose content is larger than 2GB."
     ]
    }
   ],
   "source": [
    "nb_f = [100,200,300,400,500,1000,2000,3000,4000,5000,6000,7000,8000]\n",
    "for nb_features in nb_f:\n",
    "    n = NetLauncher(name_csv = 'feature_importanceRF.csv', predict_var ='GZIP-vmlinux', drop_feature = True,\n",
    "                 nb_features = nb_features, learning_rate1 = 0.5, learning_rate2 = 0.025, nb_node_layer1 = 200,\n",
    "                 nb_node_layer2 = 300, batch_size = 50, nb_epochs = 30, training_size = 0.9)\n",
    "    n.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\llesoil\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\llesoil\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Training final cost = 7.467556419579879\n",
      "Test final cost = 8.168824794090586\n",
      "Training final cost = 7.294761979061624\n",
      "Test final cost = 7.613556095655413\n",
      "Training final cost = 3.802169253860695\n",
      "Test final cost = 5.894977753119701\n",
      "Training final cost = 3.6380126884450084\n",
      "Test final cost = 5.627356377308783\n",
      "Training final cost = 3.4912043809890747\n",
      "Test final cost = 5.273831830076549\n",
      "Training final cost = 3.5432440188267957\n",
      "Test final cost = 4.977625728949256\n",
      "Training final cost = 3.422684807103613\n",
      "Test final cost = 4.610594198496446\n",
      "Training final cost = 3.352349416285321\n",
      "Test final cost = 4.351897401330263\n",
      "Training final cost = 3.229421994810245\n",
      "Test final cost = 4.142475514308266\n",
      "Training final cost = 3.2081116426574146\n",
      "Test final cost = 4.002325971489367\n",
      "Training final cost = 3.1431700466335686\n",
      "Test final cost = 3.9461316310841106\n",
      "Training final cost = 3.2536244880968868\n",
      "Test final cost = 3.799879571665888\n"
     ]
    }
   ],
   "source": [
    "training_size = [0.05,0.1,0.15,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9, 0.95]\n",
    "for tr_size in training_size:\n",
    "    n = NetLauncher(name_csv = 'feature_importanceRF.csv', predict_var ='GZIP-vmlinux', drop_feature = True,\n",
    "                 nb_features = 500, learning_rate1 = 0.5, learning_rate2 = 0.025, nb_node_layer1 = 200,\n",
    "                 nb_node_layer2 = 300, batch_size = 50, nb_epochs = 30, training_size = tr_size)\n",
    "    n.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final cost = 3.390261431031181\n",
      "Test final cost = 4.381952502157377\n",
      "Training final cost = 2.9059068511649606\n",
      "Test final cost = 3.8958800279575847\n",
      "Training final cost = 2.9731977104708767\n",
      "Test final cost = 3.737017141736072\n",
      "Training final cost = 3.117870247522414\n",
      "Test final cost = 3.894574399875558\n",
      "Training final cost = 3.1774745376213738\n",
      "Test final cost = 4.030858796575795\n",
      "Training final cost = 3.2308373999048547\n",
      "Test final cost = 3.839309626299402\n",
      "Training final cost = 3.3827813284984534\n",
      "Test final cost = 3.8615256262862165\n",
      "Training final cost = 28.258269320363585\n",
      "Test final cost = 28.2991026899089\n",
      "Training final cost = 5.101871824062965\n",
      "Test final cost = 5.273673587519189\n",
      "Training final cost = 28.267397566118103\n",
      "Test final cost = 28.2172663419143\n",
      "Training final cost = 28.263213458268538\n",
      "Test final cost = 28.255190175512563\n"
     ]
    }
   ],
   "source": [
    "l_rates = [0.05, 0.1, 0.3, 0.4, 0.5, 0.8, 1, 1.5, 2, 5, 10]\n",
    "for l_rate in l_rates:\n",
    "    n = NetLauncher(name_csv = 'feature_importanceRF.csv', predict_var ='GZIP-vmlinux', drop_feature = True,\n",
    "                 nb_features = 500, learning_rate1 = l_rate, learning_rate2 = 0.025, nb_node_layer1 = 200,\n",
    "                 nb_node_layer2 = 300, batch_size = 50, nb_epochs = 30, training_size = 0.9)\n",
    "    n.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final cost = 3.2703712044130775\n",
      "Test final cost = 3.9689340358195095\n",
      "Training final cost = 3.2154299451245203\n",
      "Test final cost = 3.8928224094535993\n",
      "Training final cost = 3.142849015728863\n",
      "Test final cost = 3.9720706239990564\n",
      "Training final cost = 3.138090092562823\n",
      "Test final cost = 3.8323856760626254\n",
      "Training final cost = 3.059344329263853\n",
      "Test final cost = 3.8161284729190497\n",
      "Training final cost = 3.0851977681336193\n",
      "Test final cost = 3.942578701869301\n",
      "Training final cost = 3.0874523795457276\n",
      "Test final cost = 3.824946895889614\n",
      "Training final cost = 3.099181012254982\n",
      "Test final cost = 3.752506963584734\n"
     ]
    }
   ],
   "source": [
    "l_rates = [0.005, 0.010, 0.015, 0.02, 0.025, 0.03, 0.04, 0.05]\n",
    "for l_rate2 in l_rates:\n",
    "    n = NetLauncher(name_csv = 'feature_importanceRF.csv', predict_var ='GZIP-vmlinux', drop_feature = True,\n",
    "                 nb_features = 500, learning_rate1 = 0.5, learning_rate2 = l_rate2, nb_node_layer1 = 200,\n",
    "                 nb_node_layer2 = 300, batch_size = 50, nb_epochs = 30, training_size = 0.9)\n",
    "    n.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final cost = 7.970946170569618\n",
      "Test final cost = 8.052412030489549\n",
      "Training final cost = 6.434049169222514\n",
      "Test final cost = 6.622119294560474\n",
      "Training final cost = 4.94505293674515\n",
      "Test final cost = 5.113932837610659\n",
      "Training final cost = 4.5773828367968115\n",
      "Test final cost = 4.89094649579214\n",
      "Training final cost = 3.916673104112275\n",
      "Test final cost = 4.3887487248234125\n",
      "Training final cost = 3.7624633628965\n",
      "Test final cost = 4.3794055257154545\n",
      "Training final cost = 3.20240413321965\n",
      "Test final cost = 3.879077455271845\n",
      "Training final cost = 3.1212981630494627\n",
      "Test final cost = 3.8757932743300563\n",
      "Training final cost = 3.0449113696813583\n",
      "Test final cost = 3.9034350475539332\n",
      "Training final cost = 3.0072173923254013\n",
      "Test final cost = 3.985949442438457\n"
     ]
    }
   ],
   "source": [
    "n_epochs = [1,2,5,10,15,20,25,30,40,50]\n",
    "for ne in n_epochs:\n",
    "    n = NetLauncher(name_csv = 'feature_importanceRF.csv', predict_var ='GZIP-vmlinux', drop_feature = True,\n",
    "                 nb_features = 500, learning_rate1 = 0.5, learning_rate2 = 0.025, nb_node_layer1 = 200,\n",
    "                 nb_node_layer2 = 300, batch_size = 50, nb_epochs = ne, training_size = 0.9)\n",
    "    n.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = [0.1, 0.2, 0.5, 0.8, 0.9]\n",
    "for ne in n_epochs:\n",
    "    n = NetLauncher(name_csv = 'feature_importanceRF.csv', predict_var ='vmlinux', drop_feature = True,\n",
    "                 nb_features = 6000, learning_rate1 = 0.5, learning_rate2 = 0.025, nb_node_layer1 = 200,\n",
    "                 nb_node_layer2 = 300, batch_size = 50, nb_epochs = 40, training_size = 0.9)\n",
    "    n.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from net_launcher import NetLauncher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\llesoil\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\llesoil\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Training final cost = 3.6805119295627025\n",
      "Test final cost = 7.820909113987632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.6805119295627025, 7.820909113987632)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = NetLauncher(name_csv = 'feature_importanceRF.csv', predict_var ='vmlinux', drop_feature = True,\n",
    "             nb_features = 6000, learning_rate1 = 0.5, learning_rate2 = 0.025, nb_node_layer1 = 200,\n",
    "             nb_node_layer2 = 300, batch_size = 50, nb_epochs = 40, training_size = 0.9)\n",
    "n.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final cost = 16.120749826016635\n",
      "Test final cost = 18.318688701316354\n",
      "Training final cost = 11.428150324717812\n",
      "Test final cost = 15.140350938419212\n",
      "Training final cost = 13.847978991010915\n",
      "Test final cost = 16.734735814269616\n",
      "Training final cost = 4.634296331068744\n",
      "Test final cost = 11.206879426279794\n"
     ]
    }
   ],
   "source": [
    "j_r = [0.1, 0.2, 0.5, 0.8, 0.9]\n",
    "for j in j_r:\n",
    "    for i in range(3):\n",
    "        n = NetLauncher(name_csv = 'feature_importanceRF.csv', predict_var ='vmlinux', drop_feature = True,\n",
    "                 nb_features = 6000, learning_rate1 = 0.5, learning_rate2 = 0.025, nb_node_layer1 = 200,\n",
    "                 nb_node_layer2 = 300, batch_size = 50, nb_epochs = 40, training_size = j)\n",
    "        n.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\llesoil\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\llesoil\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Training final cost = 5.047830739422984\n",
      "Test final cost = 11.756315315547196\n",
      "Training final cost = 4.722511531218238\n",
      "Test final cost = 11.177169883056827\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    n = NetLauncher(name_csv = 'feature_importanceRF.csv', predict_var ='vmlinux', drop_feature = True,\n",
    "             nb_features = 6000, learning_rate1 = 0.5, learning_rate2 = 0.025, nb_node_layer1 = 200,\n",
    "             nb_node_layer2 = 300, batch_size = 50, nb_epochs = 40, training_size = 0.2)\n",
    "    n.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final cost = 4.240680277606716\n",
      "Test final cost = 9.580247612103172\n",
      "Training final cost = 4.471294619726098\n",
      "Test final cost = 9.188819797142692\n",
      "Training final cost = 4.191451230256454\n",
      "Test final cost = 9.243426131683847\n",
      "Training final cost = 4.1146275072318055\n",
      "Test final cost = 8.258303490669832\n"
     ]
    }
   ],
   "source": [
    "j_r = [0.5, 0.8, 0.9]\n",
    "for j in j_r:\n",
    "    for i in range(3):\n",
    "        n = NetLauncher(name_csv = 'feature_importanceRF.csv', predict_var ='vmlinux', drop_feature = True,\n",
    "                 nb_features = 6000, learning_rate1 = 0.5, learning_rate2 = 0.025, nb_node_layer1 = 200,\n",
    "                 nb_node_layer2 = 300, batch_size = 50, nb_epochs = 40, training_size = j)\n",
    "        n.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\llesoil\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\llesoil\\Documents\\Anaconda3\\envs\\tensor_gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Training final cost = 3.5991378740771958\n",
      "Test final cost = 7.7462372883506445\n",
      "Training final cost = 4.026943807692631\n",
      "Test final cost = 8.312332877646321\n",
      "Training final cost = 3.6917260804614007\n",
      "Test final cost = 7.645574393479721\n",
      "Training final cost = 3.7107302046916337\n",
      "Test final cost = 7.765727926855502\n",
      "Training final cost = 3.821716905791978\n",
      "Test final cost = 7.879288590472678\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    n = NetLauncher(name_csv = 'feature_importanceRF.csv', predict_var ='vmlinux', drop_feature = True,\n",
    "             nb_features = 6000, learning_rate1 = 0.5, learning_rate2 = 0.025, nb_node_layer1 = 200,\n",
    "             nb_node_layer2 = 300, batch_size = 50, nb_epochs = 40, training_size = 0.8)\n",
    "    n.launch()\n",
    "\n",
    "for i in range(3):\n",
    "    n = NetLauncher(name_csv = 'feature_importanceRF.csv', predict_var ='vmlinux', drop_feature = True,\n",
    "             nb_features = 6000, learning_rate1 = 0.5, learning_rate2 = 0.025, nb_node_layer1 = 200,\n",
    "             nb_node_layer2 = 300, batch_size = 50, nb_epochs = 40, training_size = 0.9)\n",
    "    n.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from net_launcher import NetLauncher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final cost = 13.46515973754551\n",
      "Test final cost = 14.185186266899109\n",
      "Training final cost = 13.810403321100319\n",
      "Test final cost = 14.22312655598645\n",
      "Training final cost = 13.609216342801632\n",
      "Test final cost = 14.184242734298614\n",
      "Training final cost = 4.4675585543331895\n",
      "Test final cost = 8.72098323378874\n",
      "Training final cost = 4.69846973406232\n",
      "Test final cost = 8.819656800640667\n",
      "Training final cost = 4.610786346637684\n",
      "Test final cost = 8.68688681009023\n",
      "Training final cost = 3.907199085795361\n",
      "Test final cost = 6.586479338355686\n",
      "Training final cost = 3.9746851198051285\n",
      "Test final cost = 6.6299172359964125\n",
      "Training final cost = 4.090837391044783\n",
      "Test final cost = 6.624631173714348\n",
      "Training final cost = 3.706853798388139\n",
      "Test final cost = 5.71482129978097\n",
      "Training final cost = 3.8552822997064693\n",
      "Test final cost = 5.70549759592699\n",
      "Training final cost = 3.932765529207561\n",
      "Test final cost = 5.784004725839781\n",
      "Training final cost = 3.755546544916964\n",
      "Test final cost = 5.690634159938149\n",
      "Training final cost = 3.9021932296994803\n",
      "Test final cost = 5.47583423230959\n",
      "Training final cost = 3.708070888467457\n",
      "Test final cost = 5.403958610866381\n"
     ]
    }
   ],
   "source": [
    "j_r = [0.1, 0.2, 0.5, 0.8, 0.9]\n",
    "for j in j_r:\n",
    "    for i in range(3):\n",
    "        n = NetLauncher(name_csv = 'feature_importanceRF.csv', predict_var ='vmlinux', drop_feature = True,\n",
    "                 nb_features = 500, learning_rate1 = 0.5, learning_rate2 = 0.025, nb_node_layer1 = 200,\n",
    "                 nb_node_layer2 = 300, batch_size = 50, nb_epochs = 40, training_size = j)\n",
    "        n.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
